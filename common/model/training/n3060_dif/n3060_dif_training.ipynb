{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Order Differential Model\n",
    "\n",
    "Expands upon the simple set of features used in `n3060_std` by taking their frame-to-frame differential, the difference between the current frame and the most recent frame observed.\n",
    "\n",
    "#### *Why include differences?*\n",
    "\n",
    "The image and audio processes in the field are dependent on numerous factors (wind, time of day, weather)\n",
    "and so the value of the features generated at any one time are not actuallystationary processes,\n",
    "there are variations but the processes are the cumulative sum of these variations.\n",
    "In time-series analysis, to deal with such processes one needs to apply a differential,\n",
    "where by we use the rate of change of the features as inputs to the model.\n",
    "\n",
    "To achieve this, the model needs to have not only the most recent frame available, but also\n",
    "the previous frame. It then finds the difference between these and proceeds as with n3060_std in\n",
    "feature extraction.\n",
    "\n",
    "#### *Potential Deployment Issues*\n",
    "\n",
    "The dataset that was generated here assumes that the model can operate at a constant 30 frames per second - so the difference observed is simply the difference in the underlying processes over 1/30th of a second. If the model when deployed is not able to obtain 30 frames per second then it is likely that the results will be different to those when training the model since the differences will occur over a longer time period.\n",
    "\n",
    "A potential way to mitigate this in deployment is to scale the numerical difference by the time between frames. For example, if a the deployed frame-rate is only 15 frames per second then there is 1/15 second between frames and the feature values should therefore be halved.\n",
    "Essentially we are assuming that:\n",
    "\n",
    "$$\\frac{\\partial_{\\text{process}}}{\\partial_{\\text{time}}} = c$$\n",
    "for some constant c.\n",
    "\n",
    "Numerically this is:\n",
    "$$\\frac{\\Delta_{\\text{process}}}{\\Delta_{\\text{time}}} = c$$\n",
    "where we have $\\Delta_{\\text{time}}$ as the time between frames.\n",
    "\n",
    "#### To Replicate Results:\n",
    "Use `general_constructor.py` which is designed to generate data to be used for training models when given a certain dataset configuration (stored in the directory supplied for argument `-m`).\n",
    "\n",
    "Can also specify where to save the output csv file with `-s`, database to draw labels from `-db` and total number of frames to generate `-n`.\n",
    "```\n",
    "python common/model/training/general_constructor.py \\\n",
    "-s \"/home/matthew/Documents/GapWatch/common/model/training/n3060_dif/n3060_dif.csv\" \\\n",
    "-m \"common.model.training.n3060_dif.n3060_dif\" \\\n",
    "-db \"common/data/labels/app/frames.db\" \\\n",
    "-n \"4000\"\n",
    "```\n",
    "The frames are sampled such that each class has an equal number of frames in the output dataset (regardless of the frequency of video clips stored in the label database .db file), which should allow for fairer inference. This behaviour can be changed by editing `general_constructor.py` and changing the `frame_target` array to the desired weighting.\n",
    "\n",
    "The `general_constructor.py` file first builds an index of frames to search through, such that each class is equally represented in the data. If a class is less frequent in the labels data (as \"Danger\" was) then it samples more frames from the videos of that particular class.\n",
    "\n",
    "Once the index of frames has been constructed:\n",
    "\n",
    "* Open the .mp4 file in OpenCV and seek to the desired frame\n",
    "* Perform resizing and feature extraction according to the database configuration\n",
    "  * Eg. may need to include some previous frames to get the difference between them\n",
    "  These transformations are generally run in parallel if Intel MKL is installed as quite often they will boil down to simple matrix operations\n",
    "* Convert the .mp4 file to .wav format with ffmpeg (this is usually single threaded)\n",
    "* Seek to the desired frames in the audio file output by ffmpeg and extract the features again as defined in the database configuration file\n",
    "* Close the video and audio tracks and save this chunk to the csv file. Repeat for all labels stored in the .db file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Header: \n",
      "['label' 'video_url' 'frame' 'mean' 'd_mean' 'var' 'd_var' 'kurt' 'd_kurt'\n",
      " 'skew' 'd_skew' 'mfcc_0' 'mfcc_1' 'mfcc_2' 'mfcc_3' 'mfcc_4' 'mfcc_5'\n",
      " 'mfcc_6' 'mfcc_7' 'mfcc_8' 'mfcc_9' 'd_mfcc_0' 'd_mfcc_1' 'd_mfcc_2'\n",
      " 'd_mfcc_3' 'd_mfcc_4' 'd_mfcc_5' 'd_mfcc_6' 'd_mfcc_7' 'd_mfcc_8'\n",
      " 'd_mfcc_9']\n"
     ]
    }
   ],
   "source": [
    "# load model and data and any imports\n",
    "# imports and model specific settings\n",
    "# perform imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# standard libraries\n",
    "import importlib, argparse, os.path, os, sys, time, sqlite3, subprocess, progressbar\n",
    "\n",
    "# attempt to import the dataset configuration itself\n",
    "# Add the git root directory to python path\n",
    "# unfortunately need to do this manually because Jupyter\n",
    "# sets the current working directory to the location of the notebook .ipynb file\n",
    "git_root = \"/home/matthew/Documents/GapWatch\"\n",
    "model_module = \"common.model.training.n3060_dif.n3060_dif\"\n",
    "data_path = \"common/model/training/n3060_dif/n3060_dif.csv\"\n",
    "sys.path.insert(0,git_root)\n",
    "\n",
    "# custom helper scripts\n",
    "import common.model.training.training_utils as tu\n",
    "\n",
    "# import the model\n",
    "m = importlib.import_module(model_module)\n",
    "\n",
    "# read the data\n",
    "df = pd.read_csv(os.path.join(git_root,data_path))\n",
    "\n",
    "# headers should be the same as m.const_header()\n",
    "# to keep headers consistent throughout the project\n",
    "headers = df.columns.values\n",
    "print(\"Dataframe Header: \\n{}\".format(headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the X and y to be used for modeling\n",
    "ml_df = df.copy() # make a copy\n",
    "X_cols = headers[3:] # numeric features\n",
    "y_cols = headers[0] # \"label\"\n",
    "\n",
    "# NOTE: Perform standardisation inside the model pipeline\n",
    "# as should not use test data for mean and standard deviation estimates!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "Will consider the following models:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Valid-Test Split\n",
    "Split the frames by `video_url` as this will ensure that the unseen data doesn't contain frames from videos that the model has been exposed to. This is particularly important because some of the features are video dependent (eg. lighting, weather etc) and the model should not have seen any of those criteria if the frame is truly \"unseen\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape:  (2062, 31)\n",
      "Validation set shape:  (1018, 31)\n",
      "Test set shape:  (956, 31)\n"
     ]
    }
   ],
   "source": [
    "# get the video urls to determine unique videos\n",
    "video_urls = ml_df.loc[:,headers[1]].unique()\n",
    "\n",
    "# randomly sample\n",
    "np.random.seed(0)\n",
    "train_prop = 0.5 # save half the videos for validation run\n",
    "val_prop   = 0.25\n",
    "train_idx = np.random.choice(len(video_urls),int(np.floor(train_prop*len(video_urls))),\n",
    "                           replace=False)\n",
    "val_idx_full   = np.setdiff1d(np.arange(len(video_urls)), train_idx)\n",
    "\n",
    "val_idx = np.random.choice(val_idx_full,int(np.floor(val_prop*len(video_urls))),\n",
    "                           replace=False)\n",
    "test_idx = np.setdiff1d(val_idx_full, val_idx)\n",
    "\n",
    "# find the videos corresponding to these indices\n",
    "train_videos = video_urls[train_idx]\n",
    "val_videos   = video_urls[val_idx]\n",
    "test_videos   = video_urls[test_idx]\n",
    "\n",
    "\n",
    "ml_train = ml_df.loc[ml_df[headers[1]].isin(train_videos),:]\n",
    "ml_val   = ml_df.loc[ml_df[headers[1]].isin(val_videos)  ,:]\n",
    "ml_test  = ml_df.loc[ml_df[headers[1]].isin(test_videos) ,:]\n",
    "\n",
    "print(\"Train set shape: \", ml_train.shape)\n",
    "print(\"Validation set shape: \",ml_val.shape)\n",
    "print(\"Test set shape: \",ml_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (200 of 200) |######################| Elapsed Time: 0:00:11 Time:  0:00:11\n",
      "N/A% (0 of 200) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    }
   ],
   "source": [
    "# load scikit learn logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler # (x-mu)/sigma\n",
    "\n",
    "# hide convergence warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# show progress\n",
    "import progressbar\n",
    "\n",
    "def score_model(clf, X_test, y_test, t=False):\n",
    "    # get predictions\n",
    "    if t:\n",
    "        # user defined threshold\n",
    "        y_score = clf.predict_proba(X_test)[:,1]\n",
    "        y_hat = np.where(y_score > t, 1, 0)\n",
    "    else:\n",
    "        y_hat = clf.predict(X_test)\n",
    "    # get the confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_hat).ravel()\n",
    "    # get accuracy\n",
    "    acc  = (tn+tp)/(tn+fp+fn+tp)\n",
    "    # get sensitivity\n",
    "    sens = tp/(tp+fn)\n",
    "    # get specificity\n",
    "    spec = tn/(tn+fp)\n",
    "    return acc, sens, spec\n",
    "    \n",
    "\n",
    "# perform bootstrapping to observe which coefficients can be dropped\n",
    "b = 200\n",
    "n_train = 0.5 # train on half, test on half\n",
    "\n",
    "# storage for accuracy, sensitivity, specificity (for both train and test to check for overfitting)\n",
    "scores = np.zeros((b, 6))\n",
    "bs = ShuffleSplit(n_splits = b,\n",
    "                 random_state=0,\n",
    "                 test_size=0.5)\n",
    "conf = .9 # form a 90% confidence interval to include significant variables\n",
    "X_cols_0 = X_cols.copy() # make a copy to prevent weirdness happening\n",
    "stop_cond = False\n",
    "while not stop_cond:\n",
    "    # define the response and predictors\n",
    "    X = ml_train[X_cols_0]\n",
    "    y = np.where(ml_train[y_cols]==\"Danger\",1,0) # 1: Danger, 0 No_Danger\n",
    "    # iterate training the model and performing variable selection\n",
    "    params = np.zeros((b,1+len(X_cols_0))) # storage for coefficeints\n",
    "    iter = 0\n",
    "    with progressbar.ProgressBar(max_value=b) as bar:\n",
    "        for train_index, test_index in bs.split(X):\n",
    "            X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            # fit the classifier\n",
    "            lr_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                  ('lr', LogisticRegression(random_state=0,\n",
    "                                           solver=\"sag\"))])\n",
    "            lr_pipe.fit(X_train, y_train)\n",
    "            # Automatically tune the threshold to the training data\n",
    "            y_hat = lr_pipe.predict_proba(X_train)[:, 1]\n",
    "            p, r, thresholds = tu.precision_recall_curve(y_train, y_hat)\n",
    "            target_r = 0.95\n",
    "            t_opt = thresholds[np.argmin(np.abs(r-target_r))]\n",
    "            \n",
    "            # record the model parameters\n",
    "            params[iter,:] = np.squeeze(np.hstack((lr_pipe.named_steps['lr'].intercept_[:,None],\n",
    "                                                   lr_pipe.named_steps['lr'].coef_)))\n",
    "            # score the model on train data first\n",
    "            scores[iter,:3] = score_model(lr_pipe, X_train, y_train, t=t_opt)\n",
    "            # score the model on test data\n",
    "            scores[iter,3:] = score_model(lr_pipe, X_test, y_test, t=t_opt)\n",
    "            # iteration counter\n",
    "            bar.update(iter)\n",
    "            iter +=1\n",
    "    # check the parameters for ones that are not significantly different to zero and drop\n",
    "    # then refit the model\n",
    "    # We cannot drop the intercept!\n",
    "    up = np.percentile(params[:,1:], 100*(1-(1-conf)/2), axis=0)\n",
    "    down = np.percentile(params[:,1:], 100*(1-conf)/2, axis=0)\n",
    "    keep = np.where((up>0)&(down<0),False, True)\n",
    "    not_keep = np.where((up>0)&(down<0),True, False)\n",
    "    X_cols_1 = X_cols_0[keep]\n",
    "    print(\"Variables Kept:\\n{}\\n\".format(X_cols_1))\n",
    "    print(\"Variables Removed:\\n{}\\n\".format(X_cols_0[not_keep]))\n",
    "    # update the columns to use in the model\n",
    "    X_cols_0 = X_cols_1\n",
    "    if False in keep:\n",
    "        stop_cond = False\n",
    "    else:\n",
    "        print(\"Stopping condition reached, no more variables to drop\")\n",
    "        stop_cond = True\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# boxplot of the parameters\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "ax.boxplot(params)\n",
    "ax.axhline(y=0, c=\"b\")\n",
    "x_ax_labs = np.insert(X_cols_0,0,\"intercept\") \n",
    "ax.set_xticklabels(x_ax_labs, rotation = 90)\n",
    "\n",
    "\n",
    "# boxplot of the scorings, grouped by type if possible\n",
    "# reconfigure so that the \n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "ax.boxplot(scores[:,[0,3]])\n",
    "ax.set_xticklabels([\"Training\\nAccuracy\", \"Testing\\nAccuracy\"])\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "ax.boxplot(scores[:,[1,4]])\n",
    "ax.set_xticklabels([\"Training\\nSensitivity\", \"Testing\\nSensitivity\"])\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 4)\n",
    "ax.boxplot(scores[:,[2,5]])\n",
    "ax.set_xticklabels([\"Training\\nSpecificity\", \"Testing\\nSpecificity\"])\n",
    "\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.savefig(\"lr.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### Threshold Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix\n",
    "\n",
    "X_train, X_test = ml_train[X_cols_1], ml_val[X_cols_1]\n",
    "y_train, y_test = np.where(ml_train[y_cols]==\"Danger\",1,0), np.where(ml_val[y_cols]==\"Danger\",1,0)\n",
    "lr_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                  ('dt', LogisticRegression(random_state=0,\n",
    "                                           solver=\"sag\"))])\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "# get y_hat\n",
    "y_hat = lr_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "p, r, thresholds = tu.precision_recall_curve(y_test, y_hat)\n",
    "\n",
    "target_r = 0.95\n",
    "t_opt_lr = thresholds[np.argmin(np.abs(r-target_r))]\n",
    "print(\"Threshold to achieve Sensitivity of {}: {}\".format(target_r, t_opt_lr))\n",
    "\n",
    "tu.precision_recall_threshold(p, r, thresholds, y_hat, y_test, t= t_opt_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tu.plot_precision_recall_vs_threshold(p, r, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, auc_thresholds = tu.roc_curve(y_test, y_hat)\n",
    "print(\"Area under ROC curve: {}\".format(auc(fpr, tpr))) # AUC of ROC\n",
    "tu.plot_roc_curve(fpr, tpr, 'recall_optimized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load scikit learn logistic regression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# parallel\n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "\n",
    "X = ml_train[X_cols]\n",
    "y = np.where(ml_train[y_cols]==\"Danger\",1,0) # 1: Danger, 0 No_Danger\n",
    "\n",
    "# perform bootstrapping to observe which coefficients can be dropped\n",
    "b = 200\n",
    "n_train = 0.5 # train on half, test on half\n",
    "params = np.zeros((b,1+len(X_cols))) # storage for coefficeints\n",
    "# storage for accuracy, sensitivity, specificity (for both train and test to check for overfitting)\n",
    "nk = 10 # number of different depths to try\n",
    "depths = np.arange(nk)+1 # zero depth tree doesnt make sense\n",
    "\n",
    "scores = np.zeros((b, nk, 6))\n",
    "bs = ShuffleSplit(n_splits = b,\n",
    "                 random_state=0,\n",
    "                 test_size=0.5)\n",
    "iter = 0\n",
    "start = time.time()\n",
    "X_tr, X_te, y_tr, y_te = [], [], [], [] # storage lists\n",
    "for train_index, test_index in bs.split(X):    \n",
    "    X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    X_tr.append(X_train)\n",
    "    X_te.append(X_test)\n",
    "    y_tr.append(y_train)\n",
    "    y_te.append(y_test)\n",
    "# evaluate performance in parallel\n",
    "def return_scores(split, depths):\n",
    "    train_index = split[0]\n",
    "    test_index  = split[1]\n",
    "    scores = np.zeros((nk,6))\n",
    "    for i in range(nk):\n",
    "        # fit the classifier\n",
    "        dt_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('dt', DecisionTreeClassifier(random_state=0,\n",
    "                        max_depth=depths[i]))])\n",
    "        dt_pipe.fit(X_train, y_train)\n",
    "        # threshold optimisation\n",
    "        y_hat = dt_pipe.predict_proba(X_train)[:, 1]\n",
    "        p, r, thresholds = tu.precision_recall_curve(y_train, y_hat)\n",
    "        target_r = 0.95\n",
    "        t_opt = thresholds[np.argmin(np.abs(r-target_r))]\n",
    "        # score the model on train data first\n",
    "        scores[i,:3] = score_model(dt_pipe, X_train, y_train, t=t_opt)\n",
    "        # score the model on test data\n",
    "        scores[i,3:] = score_model(dt_pipe, X_test, y_test, t=t_opt)\n",
    "    return scores\n",
    "with Pool(os.cpu_count()) as pool:\n",
    "    res = pool.starmap(return_scores,\n",
    "                        zip(bs.split(X),\n",
    "                            repeat(depths)))\n",
    "print(\"Search Complete in {} seconds\".format(time.time()-start))\n",
    "scores = np.array(res)\n",
    "\n",
    "# get the confidence intervals\n",
    "up = np.percentile(scores, 95, axis=0)\n",
    "med = np.percentile(scores, 50, axis=0)\n",
    "lo = np.percentile(scores, 5, axis=0)\n",
    "\n",
    "# Plot 1: Accuracy with depth\n",
    "metric = \"Accuracy\"\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(3, 1, 1)\n",
    "ax.fill_between(depths, up[:,0], lo[:,0], color=\"b\")\n",
    "ax.plot(depths, med[:,0], c=\"b\", label=\"Train {}\".format(metric))\n",
    "ax.fill_between(depths, up[:,3], lo[:,3], color=\"r\")\n",
    "ax.plot(depths, med[:,3], c=\"r\", label=\"Test {}\".format(metric))\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "# Plot 2: Sensitivity with depth\n",
    "metric = \"Sensitivity\"\n",
    "ax = fig.add_subplot(3, 1, 2)\n",
    "ax.fill_between(depths, up[:,1], lo[:,1], color=\"b\")\n",
    "ax.plot(depths, med[:,1], c=\"b\", label=\"Train {}\".format(metric))\n",
    "ax.fill_between(depths, up[:,4], lo[:,4], color=\"r\")\n",
    "ax.plot(depths, med[:,4], c=\"r\", label=\"Test {}\".format(metric))\n",
    "ax.legend()\n",
    "\n",
    "# Plot 3: Specificity with depth\n",
    "metric = \"Specificity\"\n",
    "ax = fig.add_subplot(3, 1, 3)\n",
    "ax.fill_between(depths, up[:,2], lo[:,2], color=\"b\")\n",
    "ax.plot(depths, med[:,2], c=\"b\", label=\"Train {}\".format(metric))\n",
    "ax.fill_between(depths, up[:,5], lo[:,5], color=\"r\")\n",
    "ax.plot(depths, med[:,5], c=\"r\", label=\"Test {}\".format(metric))\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.savefig(\"dt.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "### Threshold Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal depth appears to be 3 as overfitting happens afterwards\n",
    "max_depth=3\n",
    "X_train, X_test = ml_train[X_cols], ml_val[X_cols]\n",
    "y_train, y_test = np.where(ml_train[y_cols]==\"Danger\",1,0), np.where(ml_val[y_cols]==\"Danger\",1,0)\n",
    "clf = DecisionTreeClassifier(random_state=0,\n",
    "                        max_depth=max_depth).fit(X_train,y_train)\n",
    "# get y_hat\n",
    "y_hat = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "p, r, thresholds = tu.precision_recall_curve(y_test, y_hat)\n",
    "\n",
    "target_r = 0.95\n",
    "t_opt_dt = thresholds[np.argmin(np.abs(r-target_r))]\n",
    "print(\"Threshold to achieve Sensitivity of {}: {}\".format(target_r, t_opt_dt))\n",
    "\n",
    "tu.precision_recall_threshold(p, r, thresholds, y_hat, y_test, t= t_opt_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Models in Pipeline\n",
    "Save the models so that they can be easily reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# config utilities\n",
    "from common.data.labels.app.config_utils import JSONPropertiesFile\n",
    "\n",
    "# logistic regression\n",
    "# make sure we can automatically select the variables in X that LR expects\n",
    "# don't want to be creating a dataframe every time this pipeline is called as that will get\n",
    "# expensive quickly\n",
    "# load in the model headers\n",
    "\n",
    "# attempt to remove any previous config or pkl files\n",
    "def cleanup(directory):\n",
    "    while os.path.isfile(directory):\n",
    "        # delete temporary file\n",
    "        # file deletions appear to fail sometimes\n",
    "        os.remove(temp_fn)\n",
    "        time.sleep(0.01)\n",
    "\n",
    "m_loc = \"common.model.training.n3060_dif.n3060_dif\" # dataset configuration file\n",
    "lr_model_store = os.path.join(git_root,\"common/model/training/n3060_dif/n3060_dif_lr.pkl\")\n",
    "\n",
    "scaler_step_name = \"scaler\"\n",
    "headers = m.const_header()\n",
    "lr_sel_headers = [True if i in X_cols_0 else False for i in headers]\n",
    "\n",
    "# write a config file\n",
    "lr_CONFIG_FILE_LOC = \"lr_config.json\"\n",
    "\n",
    "cleanup(lr_CONFIG_FILE_LOC)\n",
    "cleanup(lr_model_store)\n",
    "\n",
    "lr_default_properties = {\n",
    "    'name'        : 'lr',\n",
    "    'headers'     : headers,\n",
    "    'sel_headers' : lr_sel_headers,\n",
    "    'model_store' : lr_model_store,\n",
    "    'm_loc'       : m_loc,\n",
    "    'n_prev'      : 1,\n",
    "    'scaler'      : scaler_step_name\n",
    "}\n",
    "config_file = JSONPropertiesFile(lr_CONFIG_FILE_LOC, lr_default_properties)\n",
    "config = config_file.get()\n",
    "config_file.set(config)\n",
    "# dump the logistic regression model\n",
    "X_train, X_test = ml_train[X_cols_0], ml_val[X_cols_0] # recall that these are the variables it uses\n",
    "y_train, y_test = np.where(ml_train[y_cols]==\"Danger\",1,0), np.where(ml_val[y_cols]==\"Danger\",1,0)\n",
    "\n",
    "lr_pipe = Pipeline([(scaler_step_name, StandardScaler()),\n",
    "                  ('lr', LogisticRegression(random_state=0,\n",
    "                                           solver=\"sag\"))])\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "lr_pipe.score(X_test, y_test)\n",
    "dump(lr_pipe, lr_model_store)\n",
    "\n",
    "\n",
    "# store decision tree model\n",
    "dt_model_store = os.path.join(git_root, \"common/model/training/n3060_dif/n3060_dif_dt.pkl\")\n",
    "headers = m.const_header()\n",
    "dt_sel_headers = [True if i in X_cols else False for i in headers]\n",
    "\n",
    "# write a config file\n",
    "dt_CONFIG_FILE_LOC = \"dt_config.json\"\n",
    "\n",
    "cleanup(dt_CONFIG_FILE_LOC)\n",
    "cleanup(dt_model_store)\n",
    "\n",
    "dt_default_properties = {\n",
    "    'name'        : 'dt',\n",
    "    'headers'     : headers,\n",
    "    'sel_headers' : dt_sel_headers,\n",
    "    'model_store' : dt_model_store,\n",
    "    'm_loc'       : m_loc,\n",
    "    'n_prev'      : 1,\n",
    "    'scaler'      : scaler_step_name  \n",
    "}\n",
    "config_file = JSONPropertiesFile(dt_CONFIG_FILE_LOC, dt_default_properties)\n",
    "config = config_file.get()\n",
    "config_file.set(config)\n",
    "# dump the decision tree classifier model\n",
    "X_train, X_test = ml_train[X_cols], ml_val[X_cols] # recall that these are the variables it uses\n",
    "y_train, y_test = np.where(ml_train[y_cols]==\"Danger\",1,0), np.where(ml_val[y_cols]==\"Danger\",1,0)\n",
    "\n",
    "dt_pipe = Pipeline([(scaler_step_name, StandardScaler()),\n",
    "                 ('dt', DecisionTreeClassifier(random_state=0,\n",
    "                        max_depth=max_depth))])\n",
    "dt_pipe.fit(X_train, y_train)\n",
    "dt_pipe.score(X_test, y_test)\n",
    "dump(dt_pipe, dt_model_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Models\n",
    "Simulated live inference.\n",
    "Find a video in the test set with a Dangerous scene and observe how:\n",
    "\n",
    "1. Predictors change\n",
    "2. Model predictions change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Models:\n",
    "### Previously seen footage\n",
    "How do the models perform when used on a video that they have seen before?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a video\n",
    "import common.model.training.general_demo as demo\n",
    "# vid_url = ml_test.loc[(ml_test[headers[0]]==\"Danger\")&(ml_test[headers[2]] > 100), headers[1]].values[0]\n",
    "vid_url = ml_train.loc[(ml_train[headers[0]]==\"Danger\")&(ml_train[headers[2]] > 100), headers[1]].values[0]\n",
    "out_file = os.path.join(git_root,\"common/model/training/n3060_dif/n3060_dif_inference.csv\") # save the demo data here\n",
    "# do the demo\n",
    "print(\"Performing inference on video: \\n{}\".format(vid_url))\n",
    "demo_df = demo.inference_demo(vid_url, out_file, [dt_CONFIG_FILE_LOC,\n",
    "                                                 lr_CONFIG_FILE_LOC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column names\n",
    "demo_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_lr = demo_df[\"lr\"]\n",
    "y_hat_dt = demo_df[\"dt\"]\n",
    "frames = np.arange(len(y_hat_lr))\n",
    "y_true = np.where((frames>=6915)&(frames<=6935),1,0)\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(frames, y_true, color=\"g\", label=\"True Label\")\n",
    "ax.plot(frames, y_hat_dt, color=\"r\", label=\"dt\")\n",
    "ax.plot(frames, y_hat_lr, color=\"b\", label=\"lr\")\n",
    "ax.set_xlim((6500,7000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_lr_score = lr_pipe.predict_proba(ml_df.loc[ml_df[headers[1]]==vid_url,X_cols_0])[:,1]\n",
    "y_hat_dt_score = dt_pipe.predict_proba(ml_df.loc[ml_df[headers[1]]==vid_url,X_cols])[:,1]\n",
    "labels = ml_df.loc[ml_df[headers[1]]==vid_url,headers[0]]\n",
    "\n",
    "# convert the scores to predictions using threshold\n",
    "y_hat_lr = np.where(y_hat_lr_score>t_opt_lr,1,0)\n",
    "y_hat_dt = np.where(y_hat_dt_score>t_opt_dt,1,0)\n",
    "\n",
    "frames = ml_df.loc[ml_df[headers[1]]==vid_url, headers[2]]\n",
    "fig = plt.figure(figsize = (16,8))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.set_title(\"Video: \\n{}\\nLabel Prediction\\nfrom Training Data\".format(vid_url))\n",
    "ax.plot(frames, y_hat_lr, color=\"b\", label=\"lr prediction\")\n",
    "ax.plot(frames, y_hat_dt, color=\"r\", label=\"dt prediction\")\n",
    "ax.plot(frames, labels, color = \"g\", label=\"True\")\n",
    "ax.set_xlim((6500,7000))\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.set_title(\"Video: \\n{}\\nLabel Probabiltiy\\nfrom Training Data\".format(vid_url))\n",
    "ax.plot(frames, y_hat_lr_score, color=\"b\", label=\"lr probability\")\n",
    "ax.plot(frames, y_hat_dt_score, color=\"r\", label=\"dt probability\")\n",
    "ax.plot(frames, labels, color = \"g\", label=\"True\")\n",
    "ax.set_xlim((6000,8000))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Models:\n",
    "### Previously Unseen Footage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a video\n",
    "import common.model.training.general_demo as demo\n",
    "# vid_url = ml_test.loc[(ml_test[headers[0]]==\"Danger\")&(ml_test[headers[2]] > 100), headers[1]].values[0]\n",
    "vid_url = ml_test.loc[(ml_test[headers[0]]==\"Danger\")&(ml_test[headers[2]] > 100), headers[1]].values[0]\n",
    "out_file = os.path.join(git_root,\"common/model/training/n3060_dif/n3060_dif_inference.csv\") # save the demo data here\n",
    "# do the demo\n",
    "print(\"Performing inference on video: \\n{}\".format(vid_url))\n",
    "demo_df = demo.inference_demo(vid_url, out_file, [dt_CONFIG_FILE_LOC,\n",
    "                                                 lr_CONFIG_FILE_LOC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_lr = demo_df[\"lr\"]\n",
    "y_hat_dt = demo_df[\"dt\"]\n",
    "frames = np.arange(len(y_hat_lr))\n",
    "y_true = np.where((frames>=1686)&(frames<=2120),1,0)\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(frames, y_true, color=\"g\", label=\"True Label\")\n",
    "ax.plot(frames, y_hat_dt, color=\"r\", label=\"dt\")\n",
    "ax.plot(frames, y_hat_lr, color=\"b\", label=\"lr\")\n",
    "ax.set_xlim((1500,2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_lr_score = lr_pipe.predict_proba(ml_df.loc[ml_df[headers[1]]==vid_url,X_cols_0])[:,1]\n",
    "y_hat_dt_score = dt_pipe.predict_proba(ml_df.loc[ml_df[headers[1]]==vid_url,X_cols])[:,1]\n",
    "labels = ml_df.loc[ml_df[headers[1]]==vid_url,headers[0]]\n",
    "\n",
    "# convert the scores to predictions using threshold\n",
    "y_hat_lr = np.where(y_hat_lr_score>t_opt_lr,1,0)\n",
    "y_hat_dt = np.where(y_hat_dt_score>t_opt_dt,1,0)\n",
    "\n",
    "frames = ml_df.loc[ml_df[headers[1]]==vid_url, headers[2]]\n",
    "fig = plt.figure(figsize = (16,8))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.set_title(\"Video: \\n{}\\nLabel Prediction\\nfrom Training Data\".format(vid_url))\n",
    "ax.plot(frames, y_hat_lr, color=\"b\", label=\"lr prediction\")\n",
    "ax.plot(frames, y_hat_dt, color=\"r\", label=\"dt prediction\")\n",
    "ax.plot(frames, labels, color = \"g\", label=\"True\")\n",
    "ax.set_xlim((1500,2500))\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.set_title(\"Video: \\n{}\\nLabel Probabiltiy\\nfrom Training Data\".format(vid_url))\n",
    "ax.plot(frames, y_hat_lr_score, color=\"b\", label=\"lr probability\")\n",
    "ax.plot(frames, y_hat_dt_score, color=\"r\", label=\"dt probability\")\n",
    "ax.plot(frames, labels, color = \"g\", label=\"True\")\n",
    "ax.set_xlim((1000,3000))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
